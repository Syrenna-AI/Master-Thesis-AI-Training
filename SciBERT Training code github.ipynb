{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616343b8-fbf2-499f-9a86-a0ce02aef656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811532da-0bd3-47e2-a350-192c038cc474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "EID = 0\n",
    "YEAR = 1\n",
    "KEYWORDS = 2\n",
    "TITLE = 3\n",
    "ABSTRACT = 4\n",
    "IDR = 5\n",
    "AF = 6\n",
    "\n",
    "data_file = 'scopus_UK_author_idr'\n",
    "\n",
    "UK_data = None\n",
    "with open('Datasets/{}.csv'.format(data_file), encoding='utf-8') as f:\n",
    "    r = csv.reader(f)\n",
    "    h = next(r)\n",
    "    UK_data = [line for line in r]\n",
    "    \n",
    "def process_row(row):\n",
    "    return '. '.join((row[TITLE], row[KEYWORDS], ' '.join(row[AF].split(maxsplit=50)[:50]), row[ABSTRACT]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87bcd5-b23c-4e25-997a-c9bcb5061769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: train on full dataset\n",
    "Y = torch.LongTensor([int(row[IDR]) for row in UK_data])\n",
    "X = [process_row(row) for row in UK_data]\n",
    "\n",
    "model_tr_name = 'SciBert_full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677241b3-4454-4577-8595-10bf08383ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: train on university\n",
    "uni = 'Oxford'  # 'Oxford', 'Cambridge', 'UCL'\n",
    "with open('Datasets/{}_eids.csv'.format(uni), 'r') as f:\n",
    "    r = csv.reader(f)\n",
    "    h = next(r)\n",
    "    eids = {int(row[0]) for row in r}\n",
    "\n",
    "X, Y = [], []\n",
    "for row in UK_data:\n",
    "    eid = int(row[EID])\n",
    "    if not eid in eids:\n",
    "        continue\n",
    "    X.append(process_row(row))\n",
    "    Y.append(int(row[IDR]))\n",
    "\n",
    "model_tr_name = 'SciBert_{}'.format(uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0808fb-dd5b-4f6d-9bbc-68e74037407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: train on time period\n",
    "ranges = [(2011, 2015), (2016, 2018), (2019, 2023)]\n",
    "rng = [2]\n",
    "X, Y = [], []\n",
    "for row in UK_data:\n",
    "    yr = int(row[YEAR])\n",
    "    if yr >= ranges[rng[0]][0] and yr <= ranges[rng[-1]][-1]:\n",
    "        X.append(process_row(row))\n",
    "        Y.append(int(row[IDR]))\n",
    "\n",
    "model_tr_name = 'SciBert_{}-{}'.format(ranges[rng[0]][0], ranges[rng[-1]][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad240ae3-32fb-4c0c-a480-e8d249be027b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_model_path = 'Models/{}'.format(model_tr_name)\n",
    "\n",
    "# Load from finetuned\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(current_model_path, num_labels=2).to(device)\n",
    "\n",
    "# Load from checkpoint\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('Training/checkpoint-', num_labels=2).to(device)\n",
    "\n",
    "# Load from base\n",
    "model = AutoModelForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=2).to(device)\n",
    "\n",
    "\n",
    "# Freeze the pretrained BERT layers\n",
    "for name, param in model.named_parameters():\n",
    "\tif 'classifier' not in name:\n",
    "\t\tparam.requires_grad = False\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c145bd-0de7-4d69-a2ca-2127bd3e5a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenise the training set and create the data loaders\n",
    "try:\n",
    "    with open('Embeddings/{}{}_txt_enc'.format(model_tr_name, '_1-{}'.format(undersample_ratio) if undersample_ratio else ''), 'rb') as f:\n",
    "        print('Reading encodings... ', end='')\n",
    "        tr_enc, t_enc, tr_Y, t_Y = pickle.load(f)\n",
    "    print('Done')\n",
    "\n",
    "except:\n",
    "    if undersample_ratio:\n",
    "        X, Y = undersample(undersample_ratio, X, Y)\n",
    "        print('New ratio:', len(Y)/sum(Y), len(Y))\n",
    "    print('Building encodings... ', end='')\n",
    "    tr_X, t_X, tr_Y, t_Y = train_test_split(X, Y, test_size=0.1, random_state=21)\n",
    "    lengths_sorted = np.argsort([len(x.split()) for x in tr_X])  # Sort data by length for efficient batch padding during training\n",
    "    tr_X = np.take(tr_X, lengths_sorted).tolist()\n",
    "    tr_Y = np.take(tr_Y, lengths_sorted).tolist()\n",
    "    tr_enc = tokeniser(tr_X, truncation=True, max_length=512)\n",
    "    t_enc  = tokeniser(t_X, truncation=True, max_length=512)\n",
    "    with open('Embeddings/{}{}_txt_enc'.format(model_tr_name, '_1-{}'.format(undersample_ratio) if undersample_ratio else ''), 'wb') as f:\n",
    "        pickle.dump((tr_enc, t_enc, tr_Y, t_Y), f)\n",
    "    print('Done')    \n",
    "\n",
    "class IDRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "training_loader = IDRDataset(tr_enc, tr_Y)\n",
    "test_loader = IDRDataset(t_enc, t_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911226f-192a-4605-88d0-5782840d2103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_args = TrainingArguments(\n",
    "        output_dir='Training',\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=6,\n",
    "        fp16=True,\n",
    "        per_device_train_batch_size=64, # <4GB of VRAM\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.005,\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=800,\n",
    "        save_strategy='epoch',\n",
    "        save_steps=len(X)//4,\n",
    "        save_total_limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56ff3f-5d44-471a-963f-71ce0d95f685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = min((sum(tr_Y) / len(tr_Y)), 0.999) # Class imbalance ratio\n",
    "weighted_CE = nn.CrossEntropyLoss(weight=torch.Tensor([1/((1-r)*2), 1/(r*2)]).to(model.device))  # Weighted CE using sklearn's class imbalance formula\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs['labels']\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs['logits']\n",
    "        loss = weighted_CE(logits.view(-1, 2), labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=tr_args,\n",
    "    tokenizer=tokeniser,\n",
    "    train_dataset=training_loader,\n",
    "    eval_dataset=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb6682-a402-45f2-802d-c810b89f9dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(current_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
